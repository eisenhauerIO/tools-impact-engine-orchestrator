{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Impact Engine Loop\n",
    "\n",
    "End-to-end demonstration of the Impact Engine Orchestrator.\n",
    "\n",
    "The orchestrator runs a five-stage pipeline:\n",
    "\n",
    "1. **MEASURE (pilot)** — estimate causal effects for all initiatives\n",
    "2. **EVALUATE** — score confidence based on methodology\n",
    "3. **ALLOCATE** — select a portfolio under budget constraints\n",
    "4. **MEASURE (scale)** — re-measure selected initiatives at larger sample size\n",
    "5. **REPORT** — compare predicted vs actual returns\n",
    "\n",
    "This notebook uses the real `Measure` adapter (wrapping `impact_engine`), the\n",
    "real `Evaluate` component (from `impact_engine_evaluate`), and a mock allocate\n",
    "component. As real components are integrated, only the constructor calls\n",
    "change — the orchestrator logic stays the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import yaml\n",
    "\n",
    "from impact_engine_orchestrator.config import InitiativeConfig, PipelineConfig\n",
    "from impact_engine_orchestrator.orchestrator import Orchestrator\n",
    "from impact_engine_orchestrator.components.measure.measure import Measure\n",
    "from impact_engine_evaluate import Evaluate\n",
    "from impact_engine_orchestrator.components.allocate.mock import MockAllocate\n",
    "\n",
    "# Create working directory with products data and measure configs\n",
    "_workdir = Path(tempfile.mkdtemp())\n",
    "_products_path = _workdir / \"products.csv\"\n",
    "pd.DataFrame(\n",
    "    {\n",
    "        \"product_id\": [f\"prod_{i:03d}\" for i in range(5)],\n",
    "        \"name\": [f\"Product {i}\" for i in range(5)],\n",
    "        \"category\": [\"Electronics\"] * 5,\n",
    "        \"price\": [99.99, 149.99, 79.99, 59.99, 199.99],\n",
    "    }\n",
    ").to_csv(_products_path, index=False)\n",
    "\n",
    "_measure_config = {\n",
    "    \"DATA\": {\n",
    "        \"SOURCE\": {\n",
    "            \"type\": \"simulator\",\n",
    "            \"CONFIG\": {\n",
    "                \"path\": str(_products_path),\n",
    "                \"mode\": \"rule\",\n",
    "                \"seed\": 42,\n",
    "                \"start_date\": \"2024-01-01\",\n",
    "                \"end_date\": \"2024-01-31\",\n",
    "            },\n",
    "        },\n",
    "        \"TRANSFORM\": {\"FUNCTION\": \"aggregate_by_date\", \"PARAMS\": {\"metric\": \"revenue\"}},\n",
    "    },\n",
    "    \"MEASUREMENT\": {\n",
    "        \"MODEL\": \"interrupted_time_series\",\n",
    "        \"PARAMS\": {\"dependent_variable\": \"revenue\", \"intervention_date\": \"2024-01-15\"},\n",
    "    },\n",
    "}\n",
    "\n",
    "INITIATIVE_IDS = [\n",
    "    \"product-desc-enhancement\",\n",
    "    \"checkout-flow-optimization\",\n",
    "    \"search-relevance-tuning\",\n",
    "    \"pricing-display-test\",\n",
    "    \"recommendation-engine-v2\",\n",
    "]\n",
    "_storage_url = str(_workdir / \"storage\")\n",
    "\n",
    "\n",
    "def _measure_config_path(initiative_id):\n",
    "    path = _workdir / f\"{initiative_id}.yaml\"\n",
    "    if not path.exists():\n",
    "        with open(path, \"w\") as f:\n",
    "            yaml.dump(_measure_config, f)\n",
    "    return str(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure the Pipeline\n",
    "\n",
    "Two parameter levels:\n",
    "- **Problem-level**: `budget`, `scale_sample_size`\n",
    "- **Initiative-level**: `initiative_id`, `cost_to_scale`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initiatives = [\n",
    "    InitiativeConfig(\n",
    "        \"product-desc-enhancement\",\n",
    "        cost_to_scale=15_000,\n",
    "        measure_config=_measure_config_path(\"product-desc-enhancement\"),\n",
    "    ),\n",
    "    InitiativeConfig(\n",
    "        \"checkout-flow-optimization\",\n",
    "        cost_to_scale=25_000,\n",
    "        measure_config=_measure_config_path(\"checkout-flow-optimization\"),\n",
    "    ),\n",
    "    InitiativeConfig(\n",
    "        \"search-relevance-tuning\", cost_to_scale=20_000, measure_config=_measure_config_path(\"search-relevance-tuning\")\n",
    "    ),\n",
    "    InitiativeConfig(\n",
    "        \"pricing-display-test\", cost_to_scale=10_000, measure_config=_measure_config_path(\"pricing-display-test\")\n",
    "    ),\n",
    "    InitiativeConfig(\n",
    "        \"recommendation-engine-v2\",\n",
    "        cost_to_scale=30_000,\n",
    "        measure_config=_measure_config_path(\"recommendation-engine-v2\"),\n",
    "    ),\n",
    "]\n",
    "\n",
    "config = PipelineConfig(\n",
    "    budget=100_000,\n",
    "    scale_sample_size=5000,\n",
    "    max_workers=4,\n",
    "    initiatives=initiatives,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and Run the Orchestrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orchestrator = Orchestrator(\n",
    "    measure=Measure(initiatives=initiatives, storage_url=_storage_url),\n",
    "    evaluate=Evaluate(),\n",
    "    allocate=MockAllocate(),\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "result = orchestrator.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1 — Pilot Measurements\n",
    "\n",
    "Each initiative gets a causal effect estimate with confidence intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in result[\"pilot_results\"]:\n",
    "    print(\n",
    "        f\"{p['initiative_id']:.<40s} \"\n",
    "        f\"effect={p['effect_estimate']:.2%}  \"\n",
    "        f\"CI=[{p['ci_lower']:.2%}, {p['ci_upper']:.2%}]  \"\n",
    "        f\"model={p['model_type'].value}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 2 — Evaluation Scores\n",
    "\n",
    "Confidence scores reflect methodology quality (experiments score highest)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in result[\"evaluate_results\"]:\n",
    "    print(\n",
    "        f\"{e['initiative_id']:.<40s} confidence={e['confidence']:.2f}\"\n",
    "        f\"  return_median={e['return_median']:.2%}  cost=${e['cost']:,.0f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 3 — Allocation\n",
    "\n",
    "Select initiatives by confidence-weighted return until budget is exhausted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alloc = result[\"allocate_result\"]\n",
    "print(f\"Selected {len(alloc['selected_initiatives'])} of {len(config.initiatives)} initiatives\\n\")\n",
    "\n",
    "for iid in alloc[\"selected_initiatives\"]:\n",
    "    print(\n",
    "        f\"  {iid:.<40s} budget=${alloc['budget_allocated'][iid]:,.0f}  predicted={alloc['predicted_returns'][iid]:.2%}\"\n",
    "    )\n",
    "\n",
    "total = sum(alloc[\"budget_allocated\"].values())\n",
    "print(f\"\\nTotal allocated: ${total:,.0f} / ${config.budget:,.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 4 & 5 — Scale Measurement and Outcome Reports\n",
    "\n",
    "Selected initiatives are re-measured at `scale_sample_size=5000`.\n",
    "The outcome report compares pilot predictions against scale actuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for report in result[\"outcome_reports\"]:\n",
    "    print(f\"{report['initiative_id']}\")\n",
    "    print(f\"  Predicted: {report['predicted_return']:.2%}\")\n",
    "    print(f\"  Actual:    {report['actual_return']:.2%}\")\n",
    "    print(f\"  Error:     {report['prediction_error']:+.2%}\")\n",
    "    print(f\"  Confidence: {report['confidence_score']:.2f} ({report['model_type'].value})\")\n",
    "    print(f\"  Samples:   {report['sample_size_pilot']} → {report['sample_size_scale']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determinism Check\n",
    "\n",
    "The simulator uses a fixed seed, so repeated runs produce identical results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result2 = orchestrator.run()\n",
    "assert result[\"pilot_results\"] == result2[\"pilot_results\"]\n",
    "assert result[\"outcome_reports\"] == result2[\"outcome_reports\"]\n",
    "print(\"Determinism verified — identical results across runs.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
