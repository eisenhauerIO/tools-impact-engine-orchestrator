{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Impact Engine Loop\n",
    "\n",
    "End-to-end demonstration of the Impact Engine Orchestrator.\n",
    "\n",
    "The orchestrator runs a five-stage pipeline:\n",
    "\n",
    "1. **MEASURE (pilot)** — estimate causal effects for all initiatives\n",
    "2. **EVALUATE** — score confidence based on methodology\n",
    "3. **ALLOCATE** — select a portfolio under budget constraints\n",
    "4. **MEASURE (scale)** — re-measure selected initiatives at larger sample size\n",
    "5. **REPORT** — compare predicted vs actual returns\n",
    "\n",
    "This notebook uses mock components. As real components are integrated,\n",
    "only the constructor calls change — the orchestrator logic stays the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from impact_engine_orchestrator.config import InitiativeConfig, PipelineConfig\n",
    "from impact_engine_orchestrator.orchestrator import Orchestrator\n",
    "from impact_engine_orchestrator.components.measure.mock import MockMeasure\n",
    "from impact_engine_orchestrator.components.evaluate.mock import MockEvaluate\n",
    "from impact_engine_orchestrator.components.allocate.mock import MockAllocate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure the Pipeline\n",
    "\n",
    "Two parameter levels:\n",
    "- **Problem-level**: `budget`, `scale_sample_size`\n",
    "- **Initiative-level**: `initiative_id`, `cost_to_scale`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = PipelineConfig(\n",
    "    budget=100_000,\n",
    "    scale_sample_size=5000,\n",
    "    max_workers=4,\n",
    "    initiatives=[\n",
    "        InitiativeConfig(\"product-desc-enhancement\", cost_to_scale=15_000),\n",
    "        InitiativeConfig(\"checkout-flow-optimization\", cost_to_scale=25_000),\n",
    "        InitiativeConfig(\"search-relevance-tuning\", cost_to_scale=20_000),\n",
    "        InitiativeConfig(\"pricing-display-test\", cost_to_scale=10_000),\n",
    "        InitiativeConfig(\"recommendation-engine-v2\", cost_to_scale=30_000),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and Run the Orchestrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orchestrator = Orchestrator(\n",
    "    measure=MockMeasure(),\n",
    "    evaluate=MockEvaluate(),\n",
    "    allocate=MockAllocate(),\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "result = orchestrator.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1 — Pilot Measurements\n",
    "\n",
    "Each initiative gets a causal effect estimate with confidence intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in result[\"pilot_results\"]:\n",
    "    print(\n",
    "        f\"{p['initiative_id']:.<40s} \"\n",
    "        f\"effect={p['effect_estimate']:.2%}  \"\n",
    "        f\"CI=[{p['ci_lower']:.2%}, {p['ci_upper']:.2%}]  \"\n",
    "        f\"model={p['model_type']}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 2 — Evaluation Scores\n",
    "\n",
    "Confidence scores reflect methodology quality (experiments score highest)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in result[\"evaluate_results\"]:\n",
    "    print(\n",
    "        f\"{e['initiative_id']:.<40s} confidence={e['confidence']:.2f}  R_med={e['R_med']:.2%}  cost=${e['cost']:,.0f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 3 — Allocation\n",
    "\n",
    "Select initiatives by confidence-weighted return until budget is exhausted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alloc = result[\"allocate_result\"]\n",
    "print(f\"Selected {len(alloc['selected_initiatives'])} of {len(config.initiatives)} initiatives\\n\")\n",
    "\n",
    "for iid in alloc[\"selected_initiatives\"]:\n",
    "    print(\n",
    "        f\"  {iid:.<40s} budget=${alloc['budget_allocated'][iid]:,.0f}  predicted={alloc['predicted_returns'][iid]:.2%}\"\n",
    "    )\n",
    "\n",
    "total = sum(alloc[\"budget_allocated\"].values())\n",
    "print(f\"\\nTotal allocated: ${total:,.0f} / ${config.budget:,.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 4 & 5 — Scale Measurement and Outcome Reports\n",
    "\n",
    "Selected initiatives are re-measured at `scale_sample_size=5000`.\n",
    "The outcome report compares pilot predictions against scale actuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for report in result[\"outcome_reports\"]:\n",
    "    print(f\"{report['initiative_id']}\")\n",
    "    print(f\"  Predicted: {report['predicted_return']:.2%}\")\n",
    "    print(f\"  Actual:    {report['actual_return']:.2%}\")\n",
    "    print(f\"  Error:     {report['prediction_error']:+.2%}\")\n",
    "    print(f\"  Confidence: {report['confidence_score']:.2f} ({report['model_type']})\")\n",
    "    print(f\"  Samples:   {report['sample_size_pilot']} → {report['sample_size_scale']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determinism Check\n",
    "\n",
    "Mock components are seeded by `initiative_id`, so repeated runs produce identical results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result2 = orchestrator.run()\n",
    "assert result[\"pilot_results\"] == result2[\"pilot_results\"]\n",
    "assert result[\"outcome_reports\"] == result2[\"outcome_reports\"]\n",
    "print(\"Determinism verified — identical results across runs.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
